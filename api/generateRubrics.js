// /api/generateRubrics.js
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: process.env.API_KEY,
  baseURL: process.env.BASE_URL
});

export default async function handler(req, res) {
  if (req.method !== "POST") {
    res.setHeader("Allow", "POST");
    return res.status(405).json({ error: "Method not allowed" });
  }

  try {
    let { taskPrompt, responseText, highlights = [], extras = "" } = req.body;

    // Process highlights:
    // - Red â†’ split into negative (original) + positive (correction)
    // - Green â†’ positive
    const processedHighlights = [];
    for (const h of highlights) {
      if (h.color === "red" && h.comment) {
        processedHighlights.push({
          type: "negative",
          text: h.text,
          correction: h.comment
        });
        processedHighlights.push({
          type: "positive",
          text: h.comment
        });
      } else {
        processedHighlights.push({
          type: "positive",
          text: h.text,
          comment: h.comment || ""
        });
      }
    }

    const systemPrompt = `
You are an expert rubric architect. Your task is to generate a flat, numbered list of rubric criteria that grades whether a modelâ€™s response satisfies the requirements of a given prompt.

Rubrics are answer keys with weights. They must be atomic, specific, self-contained, outcome-only, non-redundant, and comprehensive.

ğŸš« Hard Prohibitions

âŒ Do not write process/reasoning criteria (e.g., â€œcomputes using formula,â€ â€œfilters dataset,â€ â€œsorts by descending orderâ€).

âŒ Do not group items (e.g., â€œfor each player,â€ â€œall rows correctâ€).

âŒ Do not use vague words (e.g., â€œcorrectly reports,â€ â€œmentions correlationâ€).

âŒ Do not reference other criteria (e.g., â€œsee aboveâ€).

âŒ Do not skip values: if a value is missing, insert a placeholder with double curly braces, e.g., {{p_value}}.

âœ… Strict Rules for Criteria
1. Format

Output must be a flat numbered list.

No markdown, no headings, no commentary.

2. Atomicity

One fact or artefact per criterion.

Example:

âœ… â€œReports Pearsonâ€™s r = âˆ’0.3039.â€

âŒ â€œReports Pearsonâ€™s r and explains significance.â€

3. Self-contained

Each criterion must be understandable in isolation.

Do not use â€œas in criterion 2â€ or â€œsee above.â€

Example:

âœ… â€œReports the 7th prime number as 17.â€

âŒ â€œReports the next prime number correctly.â€

4. Specificity

Use exact values, names, labels, or categories.

Example:

âœ… â€œReports that Patrick Ricard appears with 4 seasons.â€

âŒ â€œReports number of seasons for each player.â€

5. Outcome-only

Grade only what appears in the final output (tables, rows, values, plots, lists, labels).

Forbidden verbs: computes, calculates, derives, defines, selects, filters, applies, determines, sorts.

Allowed verbs: States, Reports, Provides, Identifies, Includes.

6. Stacked rubrics (lists â‰¥ 10 items)

Do not grade all list elements.

Instead, create spot-checks (~20% of items) distributed across beginning, middle, and end.

Example (prime numbers prompt):

âœ… â€œReports that the 1st prime number is 2.â€

âœ… â€œReports that the 7th prime number is 17.â€

âœ… â€œReports that the 10th prime number is 29.â€

âœ… â€œReports that the 15th prime number is 47.â€

7. Tables

Require table structure (row count, required columns).

Add spot-check criteria for row values.

Example:

âœ… â€œProvides a table with exactly 20 rows and the following columns: player name, number of seasons, average offensive yards, average defensive impact, balance score.â€

8. Plots

Always include a criterion for semantic equivalence to the reference plot.

Add separate atomic checks for axes, labels, categories, and ordering.

Ignore style differences (color, fonts, line thickness) unless explicitly requested.

Example:

âœ… â€œProvides a scatter plot with average offensive yards on the x-axis.â€

âœ… â€œScatter plot is semantically the same as the reference plot.â€

9. Comprehensiveness

Cover:

All explicit asks.

Implicit requirements (e.g., exclusions, constraints).

Observed model failures (e.g., wrong row count).

10. Non-redundancy

Each fact/artefact appears once only.

Do not double-grade (e.g., â€œReports correlation coefficientâ€ and â€œReports r-valueâ€ separately).

11. Placeholders

If the model response fails to include a required value, insert {{placeholder_name}}.

Example:

â€œReports average rainfall in July as {{avg_rainfall_july}}.â€

12. Weights

Assign points based on importance:

30â€“40 â†’ Critical factual correctness (numeric values, named entities).

20â€“30 â†’ Major structure (row counts, required plots, table presence).

10â€“20 â†’ Secondary details (axis labels, ordering, highlights, legends).

5â€“15 â†’ Nice-to-have depth or nuance.

1â€“5 â†’ Reasoning steps (only if explicitly required).

13. Phrasing

Each criterion must begin with:

â€œStatesâ€¦â€

â€œReportsâ€¦â€

â€œProvidesâ€¦â€

â€œIdentifiesâ€¦â€

â€œIncludesâ€¦â€

14. Scoring

Each item must end with one of the following:

â€œ<points> points Â· must have criteriaâ€

â€œ<points> points Â· nice to have criteriaâ€

ğŸ“Š Examples
Example A: Statistical Prompt

Prompt: â€œCalculate Pearsonâ€™s correlation between income and crime.â€
Rubric:

Reports Pearsonâ€™s r = âˆ’0.3039. 35 points Â· must have criteria

Reports p-value = 0.0001. 30 points Â· must have criteria

States that the correlation is negative (higher income â†’ fewer incidents). 20 points Â· must have criteria

Example B: List Prompt (stacked)

Prompt: â€œList the first 15 prime numbers.â€
Rubric:

Reports that the 1st prime number is 2. 30 points Â· must have criteria

Reports that the 7th prime number is 17. 30 points Â· must have criteria

Reports that the 10th prime number is 29. 30 points Â· must have criteria

Reports that the 15th prime number is 47. 30 points Â· must have criteria

Example C: Table + Plots

Prompt: â€œReport the top 20 players by balance score and visualize results.â€
Rubric:

Provides a table with exactly 20 rows.

Includes the column â€œplayer name.â€

Includes the column â€œnumber of seasons.â€

Includes the column â€œaverage offensive yards.â€

Includes the column â€œaverage defensive impact.â€

Includes the column â€œbalance score.â€

Reports that Patrick Ricard appears with 4 seasons, 38.600000 average offensive yards, 5.600000 average defensive impact, and a balance score of 6.892857. 35 points Â· must have criteria

Reports that Jesse James appears with 3 seasons, 258.500000 average offensive yards, 2.750000 average defensive impact, and a balance score of 94.000000. 35 points Â· must have criteria

Reports that Josh Oliver appears with 3 seasons, 194.800000 average offensive yards, 2.000000 average defensive impact, and a balance score of 97.400000. 35 points Â· must have criteria

Provides a scatter plot with average offensive yards on the x-axis. 20 points Â· must have criteria

Provides a scatter plot with average defensive impact on the y-axis. 20 points Â· must have criteria

Scatter plot is semantically the same as the reference. 25 points Â· must have criteria

Provides a heatmap showing correlations between all numeric offensive and defensive stats. 25 points Â· must have criteria

Heatmap is semantically the same as the reference. 20 points Â· must have criteria

Provides a bar chart ranking the 20 players by balance score in ascending order. 25 points Â· must have criteria

Labels each bar with the exact balance score. 20 points Â· must have criteria

Bar chart is semantically the same as the reference. 20 points Â· must have criteria
`.trim();

    const userPrompt = `
TASK PROMPT
${taskPrompt || "(none provided)"}

MODEL RESPONSE
${responseText}

HIGHLIGHTS (with corrections applied)
${JSON.stringify(processedHighlights, null, 2)}

EXTRA NOTES
${extras}

INSTRUCTIONS
- Red highlights: original text = negative criterion, correction = positive â€œmust haveâ€.
- Green highlights: positive â€œmust haveâ€ unless explicitly optional.
- Cover all explicit asks in taskPrompt + corrections.
- Output as a flat numbered list only.
`.trim();

    const completion = await client.chat.completions.create({
      model: process.env.MODEL || "gpt-4o",
      temperature: 0,
      messages: [
        { role: "system", content: systemPrompt },
        { role: "user", content: userPrompt }
      ]
    });

    const rubrics = completion.choices[0]?.message?.content?.trim() || "";

    res.status(200).json({
      rubrics: rubrics || "No rubrics generated.",
      modelUsed: process.env.MODEL || "gpt-4o"
    });

  } catch (e) {
    console.error("Handler error:", e);
    res.status(500).json({ error: e.message || "Internal error" });
  }
}
